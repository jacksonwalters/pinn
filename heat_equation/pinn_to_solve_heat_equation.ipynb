{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba3b39d-3f3f-4e9c-b516-cb32aa491758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Define the PINN model\n",
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, layers, lb, ub, alpha):\n",
    "        \"\"\"\n",
    "        layers: list containing the number of neurons for each layer (input, hidden, output)\n",
    "        lb: lower bound for x and t (e.g., np.array([x_min, t_min]))\n",
    "        ub: upper bound for x and t (e.g., np.array([x_max, t_max]))\n",
    "        alpha: diffusivity coefficient in the heat equation\n",
    "        \"\"\"\n",
    "        super(PINN, self).__init__()\n",
    "        self.lb = tf.convert_to_tensor(lb, dtype=tf.float32)\n",
    "        self.ub = tf.convert_to_tensor(ub, dtype=tf.float32)\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.hidden_layers = []\n",
    "        for width in layers[1:-1]:\n",
    "            self.hidden_layers.append(tf.keras.layers.Dense(width, activation=tf.nn.tanh,\n",
    "                                                              kernel_initializer='glorot_normal'))\n",
    "        self.out_layer = tf.keras.layers.Dense(layers[-1], activation=None,\n",
    "                                               kernel_initializer='glorot_normal')\n",
    "    \n",
    "    def call(self, X):\n",
    "        # X is expected to be a tensor with shape (N, 2): [x, t]\n",
    "        H = 2.0 * (X - self.lb) / (self.ub - self.lb) - 1.0  # feature scaling to [-1,1]\n",
    "        for layer in self.hidden_layers:\n",
    "            H = layer(H)\n",
    "        u = self.out_layer(H)\n",
    "        return u\n",
    "\n",
    "    def pde_residual(self, X):\n",
    "        \"\"\"\n",
    "        Computes the PDE residual: u_t - alpha*u_xx\n",
    "        \"\"\"\n",
    "        with tf.GradientTape(persistent=True) as tape2:\n",
    "            tape2.watch(X)\n",
    "            with tf.GradientTape(persistent=True) as tape1:\n",
    "                tape1.watch(X)\n",
    "                u = self.call(X)\n",
    "            # X has two components: x and t\n",
    "            u_x = tape1.gradient(u, X)[:, 0:1]  # derivative with respect to x\n",
    "            u_t = tape1.gradient(u, X)[:, 1:2]  # derivative with respect to t\n",
    "        u_xx = tape2.gradient(u_x, X)[:, 0:1]   # second derivative with respect to x\n",
    "        del tape1, tape2\n",
    "\n",
    "        # PDE: u_t - alpha * u_xx = 0\n",
    "        f = u_t - self.alpha * u_xx\n",
    "        return f\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Domain bounds (e.g., x in [0,1], t in [0,1])\n",
    "lb = np.array([0.0, 0.0])\n",
    "ub = np.array([1.0, 1.0])\n",
    "alpha = 0.01  # thermal diffusivity\n",
    "\n",
    "# Neural network architecture: 2 inputs, 3 hidden layers with 20 neurons each, 1 output.\n",
    "layers = [2, 20, 20, 20, 1]\n",
    "\n",
    "# Instantiate the model\n",
    "pinn_model = PINN(layers, lb, ub, alpha)\n",
    "\n",
    "# Define optimizers and loss functions\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "@tf.function\n",
    "def train_step(X_collocation, X_data, u_data):\n",
    "    \"\"\"\n",
    "    X_collocation: collocation points in the (x,t) domain where PDE residual is enforced.\n",
    "    X_data: points with known u values (from initial and boundary conditions).\n",
    "    u_data: corresponding u values.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute PDE residual loss (physics loss)\n",
    "        f = pinn_model.pde_residual(X_collocation)\n",
    "        loss_f = tf.reduce_mean(tf.square(f))\n",
    "        \n",
    "        # Compute data loss for initial and boundary conditions\n",
    "        u_pred = pinn_model(X_data)\n",
    "        loss_data = tf.reduce_mean(tf.square(u_pred - u_data))\n",
    "        \n",
    "        # Total loss: weight factors can be tuned as needed.\n",
    "        loss = loss_f + loss_data\n",
    "\n",
    "    gradients = tape.gradient(loss, pinn_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, pinn_model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Example: Generate training data\n",
    "# Collocation points in the domain for enforcing the PDE\n",
    "N_collocation = 1000\n",
    "X_collocation = np.random.rand(N_collocation, 2)  # random points in [0,1]x[0,1]\n",
    "\n",
    "# Initial and boundary condition data points\n",
    "# For example, initial condition: u(x,0) = sin(pi*x)\n",
    "N_data = 200\n",
    "x_data = np.linspace(0, 1, N_data)[:, None]\n",
    "t_data = np.zeros_like(x_data)\n",
    "X_data = np.hstack((x_data, t_data))\n",
    "u_data = np.sin(np.pi * x_data)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    loss_value = train_step(tf.convert_to_tensor(X_collocation, dtype=tf.float32),\n",
    "                              tf.convert_to_tensor(X_data, dtype=tf.float32),\n",
    "                              tf.convert_to_tensor(u_data, dtype=tf.float32))\n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss_value.numpy()}\")\n",
    "\n",
    "# After training, you can use pinn_model.predict(...) to evaluate the solution.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
